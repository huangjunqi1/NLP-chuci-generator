# 楚辞生成

## 初步计划
### 输入
~~根据不同输入方式，为了能应用上数据集，我们应采用不同的输出过程。~~
~~1. **输入关键词**。如此我们则要先根据关键词生成现代文本，再把文本翻译成楚辞~~
2. **输入开头几个字**。这样的话我们可以直接生成楚辞。

### 输出
逐句输出一段楚辞，句数random

### 基本思路
使用transformers预训练的**tokenizer**，或者手动构建词表也行~~（若选择第一种输入，我们也可以用别人的文本生成模型）~~
然后采用**seq2seq**的**attention**模型，先使用古诗古文进行预训练。再采用楚辞进行fine-tuning。

### 问题与初步应对
1. **韵律与平仄：** 或许能自己学会,训练时应该生成到句号而不是逗号为止。
2. **字数：** 训练时的应对见下，生成时或可在567间随机截断
3. **楚地方言与生僻字**：在训练时将生僻字先替换为同义乃至同长的翻译，在生成时将翻译再换回生僻字
4. **训练集少：** 先用古诗古文预训练

## 具体分析
数据标注、存储、映射 
seq2seq模型，attention打分、合成 
预训练、训练、评估函数
生成、注释 
### 数据预处理
用于预训练的诗：不同诗体混杂，一行一首，保留标点，当作终止符。
每个batch中的句数应该相同，字数应该padding至相同
其余似乎和助教样例相同

### 模型训练
以batch为单位
与藏头诗区别：
1. 一个batch中生成到终止符的字数不同———while循环，当所有句子都生成完则标志完成，再手动将每句结束符后padding成规定字数
2. Attention忽略padding：手动修改打分矩阵？
3. 损失函数接受不同长度：将target集padding，交叉熵采用ignore_index以忽略padding

### 输出
同藏头诗？外加随机指定句数。

